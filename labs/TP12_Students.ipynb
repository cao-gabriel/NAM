{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP10 (Student version): a recommender system\n",
    "\n",
    "We can use the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this practical work is to make a basic recommender system, and use it on a Movielens dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the rating data extracted from MovieLens http://lioneltabourier.fr/documents/rating_list.txt\n",
    "\n",
    "This file is organised as follows:\n",
    "\n",
    "<pre>\n",
    "user_id   movie_id   rating\n",
    "</pre>\n",
    "\n",
    "It contains 100836 ratings of 9724 movies by 610 different users. Ratings on MovieLens goes from 0.5 to 5.\n",
    "\n",
    "The corresponding movie index is available there http://lioneltabourier.fr/documents/movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Select **randomly** 1% of the ratings (so 1008 ratings). This will be your test set for the rest of this lab: these ratings are considered as unknown, and we aim at predicting them with the learning set which is the remaining 99% ratings.\n",
    "\n",
    "Create two files, one containing the learning ratings, another containing the test ratings (please join them to the .ipynb file when sending your TP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(rating_list):\n",
    "    with open(rating_list, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        test_set_length = len(lines) // 100\n",
    "        test_set = []\n",
    "        while test_set_length > 0:\n",
    "            random_choice = random.randint(0, len(lines) - 1)\n",
    "            test_set.append(lines.pop(random_choice))\n",
    "            test_set_length -= 1\n",
    "        with open(\"test_set.txt\", \"w\") as test_set_file:\n",
    "            for line in test_set:\n",
    "                test_set_file.write(line)\n",
    "        with open(\"learning_set.txt\", \"w\") as learning_set_file:\n",
    "            for line in lines:\n",
    "                learning_set_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation(\"res/rating_list.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: benchmark recommender \n",
    "\n",
    "The benchmark recommender that you will create works as follows: for a user $u$ and an item $i$, the predicted score is\n",
    "\n",
    "$$ r^*(u,i) = \\overline{r} + ( \\overline{r(u)} - \\overline{r}) + ( \\overline{r(i)} - \\overline{r})$$\n",
    "\n",
    "$\\overline{r}$ is the average rating over the whole learning dataset.\n",
    "\n",
    "$\\overline{r(u)}$ is the average rating over the learning dataset of user $u$. In case $u$ is not present in the learning set, consider that $\\overline{r(u)} = \\overline{r}$.\n",
    "\n",
    "$\\overline{r(i)}$ is the average rating over the learning dataset of item $i$. In case $i$ is not present in the learning set, consider that $\\overline{r(i)} = \\overline{r}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Load the learning data in memory.\n",
    "\n",
    "Clue: an adequate format for the rest of this TP is to create two dictionaries of lists (warning: a dictionary of sets won't work): \n",
    "\n",
    "1) keys = user ids , values = list of ratings \n",
    "\n",
    "2) keys = item ids , values = list of ratings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(learning_data):\n",
    "    user_set, movie_set = {}, {}\n",
    "    with open(learning_data, \"r\") as file:\n",
    "        for line in file:\n",
    "            user, movie, rating = line.split()\n",
    "            user, movie, rating = int(user), int(movie), float(rating)\n",
    "            if user not in user_set:\n",
    "                user_set[user] = []\n",
    "            if movie not in movie_set:\n",
    "                movie_set[movie] = []\n",
    "            user_set[user].append(rating)\n",
    "            movie_set[movie].append(rating)\n",
    "    return user_set, movie_set\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set, item_set = load_data(\"learning_set.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Create a function which given a user $u$ and an item $i$ returns the value of $r^*(u,i)$ computed on the learning set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_score(my_user, my_item, user_set, movie_set):\n",
    "    rating_sum, rating_count = 0, 0\n",
    "    for user in user_set:\n",
    "        rating_sum += sum(user_set[user])\n",
    "        rating_count += len(user_set[user])\n",
    "    average_rating = rating_sum / rating_count\n",
    "    user_rating = sum(user_set[my_user]) / len(user_set[my_user]) if my_user in user_set else average_rating\n",
    "    item_rating = sum(movie_set[my_item]) / len(movie_set[my_item]) if my_item in movie_set else average_rating\n",
    "    return user_rating + item_rating - average_rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_score(610, 170875, user_set, item_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: evaluation\n",
    "\n",
    "Now that we have a prediction process, we evaluate its performances on the rating set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "For each rating in the test set, compute the rating predicted by the function defined above and compare it to the actual score. If an item has not been rated in the learning set or a user has made no rating in the learning set, don't do any prediction.\n",
    "\n",
    "To present your results, you can print them in the form:\n",
    "\n",
    "<pre>\n",
    "user_id item_id real_rating predicted_rating\n",
    "</pre>\n",
    "\n",
    "At first sight, what is your opinion about the ratings that you obtained?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(user_set, item_set, test_file):\n",
    "    my_evaluations = []\n",
    "    with open(test_file, \"r\") as file:\n",
    "        for line in file:\n",
    "            user, item, real_rating = line.split()\n",
    "            user, item, real_rating = int(user), int(item), float(real_rating)\n",
    "            if user in user_set and item in item_set:\n",
    "                predicted_rating = predict_score(user, item, user_set, item_set)\n",
    "                print(\"{} {} {} {}\".format(user, item, real_rating, round(predicted_rating,2)))\n",
    "                my_evaluations.append((user, item, real_rating, predicted_rating))\n",
    "    return my_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_evaluations = evaluate(user_set, item_set, \"test_set.txt\")\n",
    "for user, item, real_rating, predicted_rating in my_evaluations:\n",
    "    print(\"{} {} {} {}\".format(user, item, real_rating, round(predicted_rating,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Using the previous question, compute the _Root Mean Square Error_, as defined in the course for the whole set of predictions:\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{\\sum _{k=1} ^K (r^*_k - r_k)^2 }{K}} $$\n",
    "\n",
    "Here $K$ is the number of predictions, $ r^*_k $ the predicted rating,  $ r_k $ the real rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(evaluation):\n",
    "    return math.sqrt(sum([pow(predicted - real, 2) for _,_, real, predicted in evaluation]) / len(evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_rmse(my_evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
