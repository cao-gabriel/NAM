{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP10 (Student version): a recommender system\n",
    "\n",
    "We can use the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this practical work is to make a basic recommender system, and use it on a Movielens dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the rating data extracted from MovieLens http://lioneltabourier.fr/documents/rating_list.txt\n",
    "\n",
    "This file is organised as follows:\n",
    "\n",
    "<pre>\n",
    "user_id   movie_id   rating\n",
    "</pre>\n",
    "\n",
    "It contains 100836 ratings of 9724 movies by 610 different users. Ratings on MovieLens goes from 0.5 to 5.\n",
    "\n",
    "The corresponding movie index is available there http://lioneltabourier.fr/documents/movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Select **randomly** 1% of the ratings (so 1008 ratings). This will be your test set for the rest of this lab: these ratings are considered as unknown, and we aim at predicting them with the learning set which is the remaining 99% ratings.\n",
    "\n",
    "Create two files, one containing the learning ratings, another containing the test ratings (please join them to the .ipynb file when sending your TP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(rating_list):\n",
    "    with open(rating_list, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        test_set_length = len(lines) // 100\n",
    "        test_set = []\n",
    "        while test_set_length > 0:\n",
    "            random_choice = random.randint(0, len(lines) - 1)\n",
    "            test_set.append(lines.pop(random_choice))\n",
    "            test_set_length -= 1\n",
    "        with open(\"test_set.txt\", \"w\") as test_set_file:\n",
    "            for line in test_set:\n",
    "                test_set_file.write(line)\n",
    "        with open(\"learning_set.txt\", \"w\") as learning_set_file:\n",
    "            for line in lines:\n",
    "                learning_set_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation(\"res/rating_list.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: benchmark recommender \n",
    "\n",
    "The benchmark recommender that you will create works as follows: for a user $u$ and an item $i$, the predicted score is\n",
    "\n",
    "$$ r^*(u,i) = \\overline{r} + ( \\overline{r(u)} - \\overline{r}) + ( \\overline{r(i)} - \\overline{r})$$\n",
    "\n",
    "$\\overline{r}$ is the average rating over the whole learning dataset.\n",
    "\n",
    "$\\overline{r(u)}$ is the average rating over the learning dataset of user $u$. In case $u$ is not present in the learning set, consider that $\\overline{r(u)} = \\overline{r}$.\n",
    "\n",
    "$\\overline{r(i)}$ is the average rating over the learning dataset of item $i$. In case $i$ is not present in the learning set, consider that $\\overline{r(i)} = \\overline{r}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Load the learning data in memory.\n",
    "\n",
    "Clue: an adequate format for the rest of this TP is to create two dictionaries of lists (warning: a dictionary of sets won't work): \n",
    "\n",
    "1) keys = user ids , values = list of ratings \n",
    "\n",
    "2) keys = item ids , values = list of ratings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(learning_data):\n",
    "    user_set, movie_set = {}, {}\n",
    "    with open(learning_data, \"r\") as file:\n",
    "        for line in file:\n",
    "            user, movie, rating = line.split()\n",
    "            user, movie, rating = int(user), int(movie), float(rating)\n",
    "            if user not in user_set:\n",
    "                user_set[user] = []\n",
    "            if movie not in movie_set:\n",
    "                movie_set[movie] = []\n",
    "            user_set[user].append(rating)\n",
    "            movie_set[movie].append(rating)\n",
    "    return user_set, movie_set\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set, item_set = load_data(\"learning_set.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Create a function which given a user $u$ and an item $i$ returns the value of $r^*(u,i)$ computed on the learning set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_score(my_user, my_item, user_set, movie_set):\n",
    "    rating_sum, rating_count = 0, 0\n",
    "    for user in user_set:\n",
    "        rating_sum += sum(user_set[user])\n",
    "        rating_count += len(user_set[user])\n",
    "    average_rating = rating_sum / rating_count\n",
    "    user_rating = sum(user_set[my_user]) / len(user_set[my_user]) if my_user in user_set else average_rating\n",
    "    item_rating = sum(movie_set[my_item]) / len(movie_set[my_item]) if my_item in movie_set else average_rating\n",
    "    return user_rating + item_rating - average_rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_score(610, 170875, user_set, item_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: evaluation\n",
    "\n",
    "Now that we have a prediction process, we evaluate its performances on the rating set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "For each rating in the test set, compute the rating predicted by the function defined above and compare it to the actual score. If an item has not been rated in the learning set or a user has made no rating in the learning set, don't do any prediction.\n",
    "\n",
    "To present your results, you can print them in the form:\n",
    "\n",
    "<pre>\n",
    "user_id item_id real_rating predicted_rating\n",
    "</pre>\n",
    "\n",
    "At first sight, what is your opinion about the ratings that you obtained?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(user_set, item_set, test_file):\n",
    "    my_evaluations = []\n",
    "    with open(test_file, \"r\") as file:\n",
    "        for line in file:\n",
    "            user, item, real_rating = line.split()\n",
    "            user, item, real_rating = int(user), int(item), float(real_rating)\n",
    "            if user in user_set and item in item_set:\n",
    "                predicted_rating = predict_score(user, item, user_set, item_set)\n",
    "                #print(\"{} {} {} {}\".format(user, item, real_rating, round(predicted_rating,2)))\n",
    "                my_evaluations.append((user, item, real_rating, predicted_rating))\n",
    "    return my_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_evaluations = evaluate(user_set, item_set, \"test_set.txt\")\n",
    "for user, item, real_rating, predicted_rating in my_evaluations:\n",
    "    print(\"{:>7} {:>7} {:>7} {:>7}\".format(user, item, real_rating, round(predicted_rating,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Using the previous question, compute the _Root Mean Square Error_, as defined in the course for the whole set of predictions:\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{\\sum _{k=1} ^K (r^*_k - r_k)^2 }{K}} $$\n",
    "\n",
    "Here $K$ is the number of predictions, $ r^*_k $ the predicted rating,  $ r_k $ the real rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(evaluation):\n",
    "    return math.sqrt(sum([pow(predicted - real, 2) for _,_, real, predicted in evaluation]) / len(evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_rmse(my_evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: user-based collaborative filtering \n",
    "\n",
    "Using the same learning and testing files as in Part1, we aim at building a collaborative filtering method to improve the results. \n",
    "\n",
    "For this purpose, we define a distance between users: $ u_1 $ and $ u_2 $ will be close if they rate movies similarly and far away if they rate movies differently.\n",
    "\n",
    "When predicting a score $ r^*_{CF}(u,i)$, we take into account this distance such that close users have more influence than distant users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: loading data\n",
    "\n",
    "### Question 1\n",
    "\n",
    "To make a collaborative filtering recommender system, we need more information than in Part1. \n",
    "\n",
    "So for Part2, create two dictionnaries of lists from the learning file:\n",
    "\n",
    "1) keys = user ids , values = list of couples (item , rating) \n",
    "\n",
    "2) keys = item ids , values = list of couples (user , rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_improved(learning_data):\n",
    "    user_set, item_set = {}, {}\n",
    "    with open(learning_data, \"r\") as file:\n",
    "        for line in file:\n",
    "            user, item, rating = line.split()\n",
    "            user, item, rating = int(user), int(item), float(rating)\n",
    "            if user not in user_set:\n",
    "                user_set[user] = {}\n",
    "            if item not in item_set:\n",
    "                item_set[item] = {}\n",
    "            user_set[user][item] = rating\n",
    "            item_set[item][user] = rating\n",
    "    return user_set, item_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set_improved, item_set_improved = load_data_improved(\"learning_set.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: computing distance\n",
    "\n",
    "The distance between users is defined as follows:\n",
    "\n",
    "$$ d(u_1,u_2) = \\frac{1}{|I_1 \\cap I_2|} \\sum _{i \\in I_1 \\cap I_2} | r(u_1,i)  - r(u_2,i)| $$\n",
    "\n",
    "where $ I_1 $ is the set of items rated by $u_1$ and $ I_2 $ is the set of items rated by $u_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Compute in a 2D matrix (you can either use numpy or a list of lists to simulate a matrix) of size 610x610 (there are 610 users in the dataset) the distance between all pairs of users.\n",
    "\n",
    "**Warning:** It is the difficult part of the lab work, as you need to make a relatively efficient code. I advise you to create two matrices, one computing $\\sum _{i \\in I_1 \\cap I_2} | r(u_1,i)  - r(u_2,i)|$ and the other computing $|I_1 \\cap I_2|$. Then go through each item using the second dictionary, and update the values in both matrices for each pair of users who have rated the same movie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(user1, user2, user_set, item_set):\n",
    "    I1, I2 = set(user_set[user1].keys()), set(user_set[user2].keys())\n",
    "    my_intersection = I1.intersection(I2)\n",
    "    if len(my_intersection) == 0:\n",
    "        return -1\n",
    "    my_distance = 0\n",
    "    for item in my_intersection:\n",
    "        my_distance += abs(item_set[item][user1] - item_set[item][user2])\n",
    "    return my_distance / len(my_intersection)\n",
    "\n",
    "\n",
    "def compute_all_distances(user_set, item_set):\n",
    "    matrix = [[0 for j in range(610)] for i in range(610)]\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix[i])):\n",
    "            if i >= j:\n",
    "                my_distance = compute_distance(i + 1, j + 1, user_set, item_set)\n",
    "                matrix[i][j], matrix[j][i] = my_distance, my_distance\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = compute_all_distances(user_set_improved, item_set_improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Using the matrix of distances, compute a dictionary which contains for each user $ u $ its average distance to other users $ \\overline{d(u)} $. \n",
    "\n",
    "Note that if a user $v$ has no common ating with user $u$, it is not taken into account in the average.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$ \\overline{d(u)} = \\frac{1}{|N(u)|} \\sum _{v \\in N(u)} d(u,v)$$\n",
    "\n",
    "where $ N(u) $ is the set of users who share at least 1 rating with $u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average(user, matrix):\n",
    "    user -= 1\n",
    "    average_distance, count = 0, 0\n",
    "    for index in range(len(matrix[user])):\n",
    "        if matrix[user][index] != -1 and index != user:\n",
    "            average_distance += matrix[user][index]\n",
    "            count += 1\n",
    "    return average_distance / count\n",
    "\n",
    "def compute_all_average(matrix):\n",
    "    my_dic = {}\n",
    "    return dict((user, compute_average(user, matrix)) for user in range(610))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_average(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: evaluation\n",
    "\n",
    "The score predicted is computed in this way\n",
    "\n",
    "$$ r^*(u,i) = \\overline{r} + ( \\overline{r(u)} - \\overline{r}) + ( \\overline{r_u(i)} - \\overline{r})$$ \n",
    "\n",
    "You can observe that this score is similar to the benchmark except for the term $ \\overline{r_u(i)} $ which is \n",
    "\n",
    "$$ \\overline{r_u(i)} = \\frac{\\sum _{v \\in U} w(u,v) r(v,i)}{\\sum _{v \\in U} w(u,v)} $$\n",
    "\n",
    "It is a weighted average of the scores of other users who have rated item $i$, the weight is based on the distance  $ w(u,v) = \\frac{\\overline{d(u)}}{d(u,v)} $ and $ w(u,v) = 1 $ if $u$ and $v$ don't share any rating. In this way, if user $u$ had no common ratings with other users in the network, we fall back on the benchmark score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "For each rating in the test set, compute the rating predicted by the function defined above and compare it to the actual score. If an item has not been rated in the learning set or a user has made no rating in the learning set, don't do any prediction.\n",
    "\n",
    "To present your results, you can print them in the form:\n",
    "\n",
    "<pre>\n",
    "user_id item_id real_rating predicted_rating\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weigh_average(my_user, my_item, item_set, matrix):\n",
    "    weighted_average = 0\n",
    "    sum_weight = 0\n",
    "    for user in item_set[my_item].keys():\n",
    "        if matrix[my_user - 1][user - 1] != -1:\n",
    "            weight = 1\n",
    "            if matrix[my_user -1][user -1] != 0:\n",
    "                weigth = compute_average(my_user, matrix) / matrix[my_user - 1][user - 1]\n",
    "            sum_weight += weight\n",
    "            weighted_average += weight * item_set[my_item][user]\n",
    "    return weighted_average / sum_weight\n",
    "        \n",
    "\n",
    "def predict_score_improved(my_user, my_item, user_set, item_set, matrix):\n",
    "    rating_sum, rating_count = 0, 0\n",
    "    for user in user_set:\n",
    "        rating_sum += sum(user_set[user].values())\n",
    "        rating_count += len(user_set[user])\n",
    "    average_rating = rating_sum / rating_count\n",
    "    user_rating = sum(user_set[my_user].values()) / len(user_set[my_user]) if my_user in user_set else average_rating\n",
    "    weighted_average = compute_weigh_average(my_user, my_item, item_set, matrix) if my_item in item_set else average_rating\n",
    "    return user_rating + weighted_average - average_rating\n",
    "\n",
    "def evaluate_improved(user_set, item_set, test_file, matrix):\n",
    "    my_evaluations = []\n",
    "    with open(test_file, \"r\") as file:\n",
    "        for line in file:\n",
    "            user, item, real_rating = line.split()\n",
    "            user, item, real_rating = int(user), int(item), float(real_rating)\n",
    "            if user in user_set and item in item_set:\n",
    "                predicted_rating = predict_score_improved(user, item, user_set, item_set, matrix)\n",
    "                #print(\"{} {} {} {}\".format(user, item, real_rating, round(predicted_rating,2)))\n",
    "                my_evaluations.append((user, item, real_rating, predicted_rating))\n",
    "    return my_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_evaluations_improved = evaluate_improved(user_set_improved, item_set_improved, \"test_set.txt\", matrix)\n",
    "for user, item, real_rating, predicted_rating in my_evaluations_improved:\n",
    "    print(\"{:>7} {:>7} {:>7} {:>7}\".format(user, item, real_rating, round(predicted_rating,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Using the previous question, compute the _Root Mean Square Error_, as defined in the course for the whole set of predictions:\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{\\sum _{k=1} ^K (r^*_{CF}(k) - r(k))^2 }{K}} $$\n",
    "\n",
    "Here $K$ is the number of predictions, $ r^*_{CF}(k) $ the predicted rating,  $ r(k) $ the real rating.\n",
    "\n",
    "You should observe only a very slight improvement to the RMSE that you have computed in Part1 with the benchmark. Do you have an idea why it is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_rmse(my_evaluations_improved)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
