{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP13 (Students version): link prediction with rankings\n",
    "\n",
    "We can use the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we use is an old online social network of hamster lovers called *hamsterer*. The links of the dataset are friendship links among users. Download from the NAM webpage the datasets :\n",
    "\n",
    "- hamsterer_train.txt , this is the training dataset that we use to order pairs of nodes (roughly 90% of the full data)\n",
    "\n",
    "- hamsterer_test.txt , this is the test dataset that is the set of \"missing links\" that we are supposed to discover using a link prediction method (roughly 10% of the data).\n",
    "\n",
    "You can load both datasets as dictionary of lists as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_loop_dupes(graph):\n",
    "    for node in graph:\n",
    "        graph[node] = list(dict.fromkeys(graph[node]))\n",
    "        try:\n",
    "            graph[node].remove(node)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "def graph_from_file(file_name):\n",
    "    graph = {}\n",
    "    with open(file_name, \"r\") as graph_file:\n",
    "        for line in graph_file:\n",
    "            try:\n",
    "                node1, node2 = [int(node) for node in line.split()]\n",
    "                if node1 not in graph:\n",
    "                    graph[node1] = []\n",
    "                graph[node1].append(node2)\n",
    "                if node2 not in graph:\n",
    "                    graph[node2]= []\n",
    "                graph[node2].append(node1)\n",
    "            except:\n",
    "                pass\n",
    "    remove_loop_dupes(graph)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = graph_from_file(\"res/hamsterer_test.txt\")\n",
    "train_set = graph_from_file(\"res/hamsterer_train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Features for link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "- Make a function that, given a pair of nodes $ (i,j) $ in the network, computes the number of Common Neighbors (CN) between these two nodes.\n",
    "\n",
    "- Do the same thing for the preferential attachment index of $ (i,j) $, we remind that\n",
    "$$ PA(i,j) = |N(i)|.|N(j)| $$\n",
    "\n",
    "- Do the same thing for the Adamic-Adar index of $ (i,j) $, we remind that\n",
    "$$ AA(i,j) = \\sum _{k \\in N(i) \\cap N(j)} \\frac{1}{log(d_k)} $$\n",
    "\n",
    "Here $ N(i) $ is the neighborhood of node $i$ and $ d_k $ the degree of node $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_CN(i, j, graph):\n",
    "    return len(set(graph[i]).intersection(graph[j]))\n",
    "\n",
    "def compute_PA(i,j, graph):\n",
    "    return len(graph[i]) * len(graph[j])\n",
    "\n",
    "def compute_AA(i,j, graph):\n",
    "    return sum([1 / math.log(len(graph[node])) for node in set(graph[i]).intersection(graph[j])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_CN(237,238,train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_PA(490,492, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_AA(490,492,train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Select 10000 pairs of nodes randomly in the training dataset.\n",
    "\n",
    "- Plot the corresponding data points in a $PA$ vs $CN$ space with a red point if the edge exists and a blue point if it doesn't (as in the course). It is relevant here to use a logscale for the $PA$ axis (for readability purposes).\n",
    "\n",
    "- How many connected pairs do you observe with this plot? What is the name of the phenomenon that we have described in the course and that this plot shows?\n",
    "\n",
    "- Do you think that a $k$-nearest neighbor method would work well with these features? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PA_CN(train_set):\n",
    "    nodes = list(train_set.keys())\n",
    "    pa_exist,cn_exist = [], []\n",
    "    pa_not, cn_not = [], []\n",
    "    for i in range(10000):\n",
    "        node1, node2 = random.choice(nodes), random.choice(nodes)\n",
    "        pa, cn = compute_PA(node1, node2, train_set), compute_CN(node1, node2, train_set)\n",
    "        if node1 in train_set[node2]:\n",
    "            pa_exist.append(pa) ; cn_exist.append(cn)\n",
    "        else:\n",
    "            pa_not.append(pa) ; cn_not.append(cn)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.scatter(pa_not, cn_not, color = \"blue\")   \n",
    "    plt.scatter(pa_exist, cn_exist, color = \"red\")\n",
    "    print(len(pa_exist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_PA_CN(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see around 70 connected pairs in the plot.\n",
    "The name of the phenomenon is overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Ranking to predict links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Write functions that allows to rank pairs of nodes by decreasing score for the four scores computed above: $CN$, $PA$, $J$ and $AA$.\n",
    "\n",
    "**Warning**: even if our dataset is rather small, we need to optimize this computation, otherwise it will be too long: we only compute the scores of pairs at distance exactly 2 (if the distance is 1 these nodes are already connected, if the distance is > 2, they share no neighbors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(train_set, compute_score):\n",
    "    scores = {}\n",
    "    for node1 in train_set:\n",
    "        for node2 in train_set[node1]:\n",
    "            for node3 in train_set[node2]:\n",
    "                if node3 > node1:\n",
    "                    scores[(node1, node3)] = compute_score(node1, node3, train_set)\n",
    "    scores = sorted(scores.items(), key=lambda x : x[1], reverse=True)\n",
    "    return dict(scores)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_ranks = rank(train_set, compute_CN)\n",
    "pa_ranks = rank(train_set, compute_PA)\n",
    "aa_ranks = rank(train_set, compute_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "- For a given number of predictions $T$ on a given ranking, make a function that computes the Precision and Recall for this prediction. We remind that\n",
    "\n",
    "$$ Pr = \\frac{\\# tp}{\\# predictions}$$ \n",
    "\n",
    "$$ Rc = \\frac{\\# tp}{\\# connected \\ pairs \\ to \\ discover} $$\n",
    "\n",
    "- Try the following values for $T$: [1000, 2000, 3000, 4000, 5000, 10000, 20000, 30000, 50000, 100000] and plot the corresponding curve in the recall-precision space for the *CN*, *PA* and *AA* index and also for a random ordering. \n",
    "\n",
    "- Which index seems to be the most efficient with these data? Does it correspond to the example of the course? Do you see any explanation for that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_recall(prediction_count, ranking, test_set):\n",
    "    predictions, tp = list(ranking.keys())[:prediction_count], 0\n",
    "    for node1, node2 in predictions:\n",
    "        if node2 in test_set and node1 in test_set[node2]:\n",
    "            tp += 1\n",
    "    return tp / prediction_count, tp / len(test_set)\n",
    "    \n",
    "def plot_recall_precision(T, ranking, test_set, legend):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for t in T:\n",
    "        precision, recall = compute_precision_recall(t, ranking, test_set)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    plt.plot(precisions, recalls, label=legend)\n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = [1000, 2000, 3000, 4000, 5000, 10000, 20000, 30000, 50000, 100000] \n",
    "plot_recall_precision(T, cn_ranks, test_set, \"CN score\")\n",
    "plot_recall_precision(T, aa_ranks, test_set, \"AA score\")\n",
    "plot_recall_precision(T, pa_ranks, test_set, \"PA score\")\n",
    "random_ranks = list(pa_ranks.items())\n",
    "random.shuffle(random_ranks)\n",
    "random_ranks = dict(random_ranks)\n",
    "plot_recall_precision(T, random_ranks, test_set, \"Random score\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these data, the most efficient seems to be the PA score, it achieved a precision of 0.040 and a recall of 0.75 whereas the other score go around a precision of around 0.030 and a recall of 0.70"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
